\begin{savequote}[75mm] 
Talk is cheap. Show me the code. \qauthor{Linus Torvalds} 
\end{savequote}

\chapter{Algorithm \label{chap4:algorithm}}

This chapter introduces and differentiates between two types of algorithms that have been evaluated from the literature in Section \ref{sec2:outlier}. The algorithms presented in Table \ref{tbl:overviewAlgorithms} are distance based algorithms. For simplicity we use Euclidean distance, as in Equation \ref{eq:euclidean}, but all algorithms also work with other metrics. For example, metrics such as the Hamming distance, which can find the number of different symbols in strings of equal length and which is often used for comparing bit-sequences \cite{citeulike:1667687}. Another distance-function is the Levenshtein distance, which is used for comparing different strings of text. The Euclidean distance between observation $a$ and $b$ is the length of the line segment connecting them, and is therefore intuitive for people to grasp \cite{Deza2009EncyclopediaofDistances}.

\begin{equation}
d({\bf a},{\bf b}) = \sqrt{\sum^{m}_{i=1} (a_{i} - b_{i})^2} \label{eq:euclidean}
\end{equation}

The distance-based algorithm is related to the concept of clustering. An outlier can be seen as an observation which is not part of a cluster. Also, the algorithms take the locality into account. For example, for the Local Outlier Factor (LOF) algorithm, which compares the density of the surrounding neighbours, it is important to note that the global density may vary a lot across the feature space. Section \ref{sec:lof} introduces the Local Outlier Factor algorithm, which introduces the concept of local densities and which has been extended and adapted by many papers. This algorithm gives a good idea what outlier detection is about. Next, the Stochastic Outlier Selection (SOS) algorithm is introduced in Section \ref{sec:sos} and it elaborates on why it is a good fit for Apache Spark and LOF is not.

For outlier detection algorithms, there is no free lunch \cite{Wolpert95nofree}, which implies that there is no algorithm which outperforms all other algorithms \cite{outlierselection}. Different algorithms excel under different conditions, depending on the characteristics of the data set.

\section{Local Outlier Factor \label{sec:lof}}

Many distance-based outlier detection algorithms are based or strongly inspired by the Local Outlier Factor algorithm. The concept is depicted in Figure \ref{fig:lof}, where the black dots are the observations in the feature space and the dashed circles are the corresponding distances to the $k^{\text{th}}$-observation which is set to $k=3$.

\begin{figure}[ht!]
\centering
\includegraphics[width=.6\textwidth]{figures/lof.png}
\caption{Local Outlier Factor with $k=3$. \label{fig:lof}}
\end{figure}

First, the distance to the $k^{\text{th}}$-nearest observations is computed. This is called the Local Reachability Distance (LDR), as defined in Equation \ref{eq:reachability}, where $k\text{-distance}(A)$ is the distance to the $k^{\text{th}}$-nearest observation of the neighbouring observation $A$. 

\begin{equation}
\text{LDR}_{k}(A) = 1 / \left( \frac{\sum_{B \in N_{k}(A)} max(k\text{-distance}(B), d(A,B))}{|N_{k}(A)|} \right) \label{eq:reachability}
\end{equation}

After computing LDR for each observation, which requires the $k$-nearest neighbours and the $k$-nearest neighbors of the $k$-nearest neighbors, the Local Outlier Factor value can be obtained using Equation \ref{eq:lof}, which essentially compares the density of the observation itself with the density of the surrounding observations.

\begin{equation}
\text{LOF}_{k}(A) = \left( \frac{\sum_{B \in N_{k}(A)} \text{LDR}(B)}{|N_{k}(A)|} \right) / \text{LDR}(A)
\label{eq:lof}
\end{equation}

The algorithm relies heavily on nearest-neighbour searches which are hard to optimize on distributed computational platforms as they require searching through and sort all the data. Therefore the algorithm is not suitable to implement on top of Apache Spark.

\section{Stochastic Outlier Selection \label{sec:sos}}

Instead of comparing local densities, as done by the Local Outlier Factor concept, the Stochastic Outlier Selection algorithm employs the concept of affinity which does comes from the field of \cite{ictdbid:2777,AISTATS09_Maaten}:
\begin{description}
  \item[Clustering] employs affinity to quantify the relationships among observations \cite{Frey2007}. For example, the concept of affinity been used for partitioning protein interactions \cite{pmid19331680} and clustering text \cite{10.1109/TKDE.2010.144}.
  \item[Dimensionality reduction] Stochastic Neighbor Embedding \cite{NIPS20022276} or t-distributed Stochastic Neighbor Embedding \cite{van2008visualizing,ictdbid:2777} is a nonlinear dimensionality reduction technique for embedding high-dimensional data into a space of two or three dimensions. The technique has been used among music analysis \cite{Hamel+al:2010} and bio-informatics \cite{journals/bioinformatics/WallachL09}.
\end{description}

The implementation of the Stochastic Outlier Selection algorithm has been done in Scala\footnote{Scala programming language \url{http://www.scala-lang.org/}} on top of Apache Spark. To determine the correct working of the algorithm, unit-tests have been written in order to ensure correct output in each stage in the algorithm. The output of the implementation is compared to the output generated by the Python implementation written by the author\footnote{SOS \url{https://github.com/jeroenjanssens/sos/blob/master/bin/sos/}}. These unit-tests uncovered a bug in the authors' script, which has been patched by submitting a pull request\footnote{Github pull request \url{https://github.com/jeroenjanssens/sos/pull/4/}}.

The algorithm consists of a series of transformations on the data as described in the original paper \cite{MSU:CSE:00:2}. These steps are elaborated in the following subsections, which describe how they are implemented.

\subsection{Distance matrix \label{subsec:distancematrix}}
The distance matrix takes the $n$ input vectors of $m$ length and transforms it to a $n \times n$ matrix by taking the pairwise distance between each vector. We employ the symmetric Euclidean distance, as defined earlier in Equation \ref{eq:euclidean}. Being symmetric, the sets' distance $d({\bf x}_{i},{\bf x}_{j})$ is equal to $d({\bf x}_{j},{\bf x}_{i})$ and the distance to self is zero $d({\bf x}_{i},{\bf x}_{i}) = 0$. The distance matrix is denoted by ${\bf D}$, each row by ${\bf D}_{i} \in \mathbb{R}^{m}$ and each element by $D_{ij} \in \mathbb{R}$.

\begin{listing}[ht!]
\begin{minted}[frame=lines]{scala}
def computeDistanceMatrixPair(data: RDD[(Long, Array[Double])]): 
                                        RDD[(Long, Array[Double])] =
    data.cartesian(data).flatMap {
      case (a: (Long, Array[Double]), b: (Long, Array[Double])) =>
        if (a._1 != b._1)
          Some(a._1, euclDistance(a._2, b._2))
        else
          None
    }.combineByKey(
        (v1) => List(v1),
        (c1: List[Double], v1: Double) => c1 :+ v1,
        (c1: List[Double], c2: List[Double]) => c1 ++ c2
      ).map {
      case (a, b) => (a, b.toArray)
    }
\end{minted}

\caption{Computing the distance matrix of a collection of feature vectors.}
\label{list:matrixDistance}
\end{listing}

The implementation given in Listing \ref{list:matrixDistance} takes the Cartesian product of the input vectors to compute the distance between ever pair of observations. Next, the pairs are mapped to compute the Euclidean distance between every pair of vectors except to itself, as this does not carry any information and is not used by the algorithm. Finally, all the vectors are combined by the unique key of each vector returning the rows of the matrix.

\subsection{Affinity matrix \label{subsec:affinity}}
The affinity matrix is obtained by transforming the distance matrix proportionally to the distance between two observations. The affinity quantifies the relationship from one observation to another. The affinity $\sigma_{i}$ for each observation is found by performing a binary search, which makes the entropy of the distribution between overall neighbors equal to the logarithm of the perplexity parameter $h$.

\begin{equation}
a_{ij} = \begin{cases} \exp(-d_{ij}^{2} / 2 \sigma_{i}^{2}) & \text{if } i \neq j\\ 0 & \text{if } i = j \end{cases}
\label{eq:affinity}
\end{equation}

The perplexity parameter is the only configurable parameter of the algorithm denoted by $h$ as in Equation \ref{eq:perplexity}. The influence of the $h$ parameter is comparable to the $k$ parameter in the $k$-nearest neighbours algorithm. It also alters the behaviour of the algorithm analogously: the higher the value $h$, the more it depends on the surrounding neighbours. One important difference is that $h \in \mathbb{R}$ and $k \in \mathbb{N}$. The perplexity value has a deep foundation in information theory, but in practice should it be tuned by the domain expert to provide a good level of outlierness \cite{MSU:CSE:00:2}.

\begin{listing}[ht!]
\begin{minted}[frame=lines]{scala}
def computeAffinityMatrix(dMatrix: RDD[(Long, Array[Double])], 
                          perplexity: Double = DEFAULT_PERPLEXITY): 
                                  RDD[(Long, DenseVector[Double])] = 
  dMatrix.map(r => (r._1, binarySearch(new DenseVector(r._2), Math.log(perplexity))))
\end{minted}
               
\caption{Transforming the distance matrix to the affinity matrix.}
\label{list:matrixAffinity}
\end{listing}

Listing \ref{list:matrixAffinity} applies the binary search on each row within the matrix which approximates the affinity. The maximum number of iterations in the binary search can be limited in order to interrupt the execution which will introduce some error, but will reduce the computational time. Similar, an accepted tolerance can be set to accept a small error in exchange for reduced computational time. The binary search iteratively bisects the interval and then selects the correct upper of lower half until the desired variances for each observation is found.

\begin{equation}
h = \{x \in \mathbb{R} | 1 \leq x \leq n-1\}
\label{eq:perplexity}
\end{equation}

The affinity matrix is obtained by applying Equation \ref{eq:affinity} on every element of the distance matrix. Perplexity is employed to adaptively set the variances which are computed using the introduced binary search, which approximates for every value the $\sigma^{2}$ so that every observation has effectively $h$ neighbours \cite{NIPS20022276}. This has to be done for every observation assuming that it has a unique place in space. 

\begin{listing}[ht!]
\begin{minted}[frame=lines]{scala}
def binarySearch(affinity: DenseVector[Double],
                 logPerplexity: Double,
                 iteration: Int = 0,
                 beta: Double = 1.0,
                 betaMin: Double = Double.NegativeInfinity,
                 betaMax: Double = Double.PositiveInfinity,
                 maxIterations: Int = DEFAULT_ITERATIONS,
                 tolerance: Double = DEFAULT_TOLERANCE): DenseVector[Double] = {

  val newAffinity = affinity.map(d => Math.exp(-d * beta))
  val sumA = sum(newAffinity)
  val hCurr = Math.log(sumA) + beta * sum(affinity :* newAffinity) / sumA
  val hDiff = hCurr - logPerplexity

  if (iteration < maxIterations && Math.abs(hDiff) > tolerance) {
    val search = if (hDiff > 0)
      (if (betaMax == Double.PositiveInfinity || betaMax == Double.NegativeInfinity)
        beta * 2.0
      else
        (beta + betaMax) / 2.0, beta, betaMax)
    else
      (if (betaMin == Double.PositiveInfinity || betaMin == Double.NegativeInfinity)
        beta / 2.0
      else
        (beta + betaMin) / 2.0, betaMin, beta)

    binarySearch(   affinity,
                    logPerplexity, 
                    iteration + 1, 
                    search._1, 
                    search._2, 
                    search._3, 
                    maxIterations, 
                    tolerance)
  }
  else
    newAffinity
}
\end{minted}
\caption{Performing a binary search to approximate the affinity for each observation.}
\label{list:approximateAffinity}
\end{listing}

Listing \ref{list:approximateAffinity} shows the recursive function used to find or approximate the perplexity of each row $x_{i}$ in the distance matrix. The recursive function eliminates mutable variables which are impossible to avoid when using using loops. The use of immutable variables comes from the functional programming aspect of Scala and makes the code less error-prone.

\subsection{Binding probability matrix \label{subsec:bindingprob}}

The binding probability matrix defines the probability of observation $x_{i}$ to $x_{j}$. The mathematical foundation, based on graph theory, computes the Stochastic Neighbour Graph and the subsequent generative process is out of scope and can be found in the original paper \cite{MSU:CSE:00:2}.

\begin{equation}
b_{ij} = \frac{a_{ij}}{\sum_{k=1}^{n}a_{ik}}
\label{eq:binding}
\end{equation}

The binding probability matrix can be applied by normalizing the affinity matrix such that $\sum_{k=1}^{n}$ sums up to $1$. Equation \ref{eq:binding} transforms the affinity matrix to the probability outlier matrix.

\begin{listing}[ht!]
\begin{minted}[frame=lines]{scala}
def computeBindingProbabilities(rows: RDD[(Long, DenseVector[Double])]): 
                                            RDD[(Long, Array[Double])] = 
  rows.map(r => (r._1, r._2 :/ sum(r._2)).toArray)
\end{minted}

\caption{Transforming the affinity matrix into the binding probability matrix.}
\label{list:matrixProbability}
\end{listing}

Listing \ref{list:matrixProbability} shows the implementation which divides each element by the sum of the vector.

\subsection{Computing outlier probabilities \label{subsec:outlierprobabilities}}

The last step is to compute the outlier probability of vector ${\bf x}_{i}$, which is given by the product of the column binding probability matrix as in Equation \ref{eq:final}.

\begin{equation}
f{_\text{SOS}}({\bf x}_{i}) \equiv \prod^{n}_{\substack{i=1\\j \neq i}}(1-b_{ji}) \label{eq:final}
\end{equation}

The implementation is given in Listing \ref{list:matrixOutput}. The implementation uses a \texttt{flatmap} to add indices to the elements of the vector. The inline if-condition offsets the indices by one to skip the diagonal of the matrix. Important to note is the \texttt{zipWithIndex} operation is applied to the local collection and not on the RDD which would trigger a subsequent Spark job.

\begin{listing}[ht!]
\begin{minted}[frame=lines]{scala}
def computeOutlierProbability(rows: RDD[(Long, Array[Double])]):
                                                    RDD[(Long, Double)] =
  rows.flatMap(r => r._2.zipWithIndex.map(p =>
    (p._2 + (if (p._2 >= r._1) 1L else 0L), p._1))).foldByKey(1.0)((a, b) => a * (1.0 - b))
\end{minted}

\caption{Computing the outlierness from the binding probability matrix.}
\label{list:matrixOutput}
\end{listing}

Finally the \texttt{foldByKey} groups all the keys and perform a fold which computes the product of each column. This last action invokes a data-shuffle, as the rows are distributed across the different worker nodes. The product represents the outlierness of each observation.