\begin{savequote}[75mm] 
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname} 
\end{savequote}


\chapter{Conclusion \label{chap6:results}}

This research will be concluded in Section \ref{sec:conclusion} using the Research Question defined in Section \ref{sec1-researchquestions}. Subsequently the further research is presented in Section \ref{sec:futherResearch}, which will provide further  uncovered research questions which have been exposed by this work.

\section{Conclusion \label{sec:conclusion}}

The introduction given in Section \ref{chap1:introduction} illustrates the growth of information in today's society. The main goal of outlier detection is to provide automated extraction of possible valuable observations within a dataset which is too big or complex to analyze by traditional statical software. To detect outliers, sometimes referred as an anomaly, exception, novelty, fault or error within these staggering amounts of data requires a scalable approach. The background of outlier detection and a variety of useful applications is given in Section \ref{sec1-motivation}, among other the use in the field of financial transactions, sensor monitoring and quality control.

The definition of outlier detection we follow; `An outlier is a data point that deviates quantitatively from the majority of the data points, according to an outlier-selection algorithm'. Outlier detection is part of the field of Knowledge Discovery and Data Mining and the computational platform to scale and distribute the algorithm is rarely taken into account. Chapter \ref{chap2:background} provides a solid background provides a solid background on outlier detection in Section \ref{sec2:outlier}. The different types of outlier detection are introduced and a set of potential algorithms is presented. Subsequently Web-scale computing is introduced in Section \ref{sec2:webscale} to provide models and tools to parallelize and scale the process of outlier detection.

Chapter \ref{chap3:architecture} introduces the architecture which allows the application to run onto a cluster of machines distributed across a data-center. The underlying pattern is the Microservice pattern which consists of suites of independently deployable services. Apache Spark is used as the computational platform which enables large-scale processing. Spark also takes care of failing nodes in the cluster by restarting the task onto another worker. Apache Kafka is used as the data-source because of it's ability to scale and divide load. Kafka acts as a message queue which integrates nicely with Spark Streaming to apply the algorithm as new messages are appended. All the services are deployed using Docker which enables fast and easy deployment on heterogeneous systems.

Starting with Research sub-question \ref{sub-req1}; \emph{`Which general-purpose outlier detection algorithms can potentially be scaled out'}. A variety of the types and important outlier detection algorithms are given in Table \ref{tbl:overviewAlgorithms}. This boils down to two suitable algorithms in Section \ref{chap4:algorithm}, i.e. Local Outlier Factor (LOF) based algorithms and the Stochastic Outlier Selection (SOS) algorithm. The SOS algorithm is chosen over the (LOF) algorithm because the LOF algorithm requires many $k$-nearest neighbour queries which are hard to implement efficiently in a map-reduce model.

Research sub-question \ref{sub-req2} focuses on the field of big-data and distributed computing to explorer the possibilities of scaling; \emph{`Which computational engines are appropriate for outlier detection in a big-data setting'}. The term `big-data' is an ambiguous terms, therefore a definition is in place, Section \ref{sec2:webscale} defines it as; `massively parallel software running on tens, hundreds, or even thousands of servers'. Many outlier detection algorithm do not take scalability into account and solely focus on the ability to detect outliers. The goal is to scale outlier detection on a large pool of easily accessible virtualized resources, which can be dynamically reconfigured to adjust to a variable load. The infrastructure given in Chapter \ref{chap3:architecture} enables to scale to algorithm by adding or removing additional worker based on load or requirements. By wrapping services inside Docker containers enables fast deployment onto a large number of machines. As the number of machines increases, it becomes more probable that one of the machines will fail, therefore availability is guaranteed using Apache Zookeeper which provides reliable distributed coordination.

The last Research sub-question \ref{sub-req3}; \emph{`How to adapt the algorithm to work with streams of data, rather than a static set'}. Data is generated faster and data real-time processing is required to act upon whats happening right now. By using the Spark Streaming library and the Apache Kafka data source this can be done. This is done in a micro-batching characteristic. The window is defined by a window length, which defines how many observations will be taken and the interval at how many observations the algorithms will be executed. This allows windows to overlap.

Based on the results in Chapter \ref{chap5:results} a number of conclusions can be drawn. We observe that:

\begin{itemize} 
    \item A distributed implementation of outlier detection works and follows the computational complexity of the algorithm $\mathcal{O}(n^{2})$ as a function of the input size.
    \item The number of partitions within the RDD needs to be tuned to the size of the input and the number of worker nodes to utilize the resources in the cluster more optimal.
    \item The configuration of Apache Spark has to be tuned to the specifications of the systems to avoid unnecessary swapping which induces significant overhead and slows down the computations.
\end{itemize}

All the computations within Spark are executed in stages. After each stage synchronization between the workers is done and data is exchanges which is required for the next stage. Based on these observations, the algorithm can be further optimized as explained in the next section.

\section{Further research \label{sec:futherResearch}}
The research presented a distributed implementation of outlier detection. This is the first public available implementations on top of Apache Spark. The problem lies in the computational complexity of the computation of the distance matrix. By taking the Cartesian product this is quadratic in respect to the input size. The architecture allows the system to scale, but when the input size gets too large, this becomes infeasible.

For further research, the following questions are still open:
\begin{itemize}
    \item Creating a more powerful primitive for the Cartesian product. The computation of the distance matrix is a special case as it takes the Cartesian product of itself, this case might be susceptible for optimization.
    \item Explore the possibility of an optimized data-structures such as the Barnes-Hut tree \cite{Barnes1986} in a distributed fashion. This has been done in a shared memory environment \cite{Dubinski1996132}.
\end{itemize}

