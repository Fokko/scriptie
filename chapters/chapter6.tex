\begin{savequote}[75mm] 
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname} 
\end{savequote}


\chapter{Conclusion \label{chap6:results}}

This research will be concluded in Section \ref{sec:conclusion} using the Research Question defined in Section \ref{sec1-researchquestions}. Subsequently, further research is presented in Section \ref{sec:futherResearch}, which will provide further uncovered research questions which have been exposed by the present work.

\section{Conclusion \label{sec:conclusion}}

The introduction given in Section \ref{chap1:introduction} illustrates the growth of information in today's society. The main goal of outlier detection is to provide automated extraction of possible valuable observations within a dataset which is too big or complex to be analyzed by traditional statical software. To detect outliers (sometimes referred to as anomalies, exceptions, novelties, faults or errors) within these staggering amounts of data, a scalable approach is required. The background of outlier detection and a variety of useful applications is presented in Section \ref{sec1-motivation}, among which its use in the fields of financial transactions, sensor monitoring and quality control.

Our definition of outlier detection is as follows: `An outlier is a data point that deviates quantitatively from the majority of the data points, according to an outlier-selection algorithm'. Outlier detection is part of the field of Knowledge Discovery and Data Mining and the computational platform to scale and distribute the algorithm is rarely taken into account. Chapter \ref{chap2:background} provides a solid background on outlier detection in Section \ref{sec2:outlier}. The different types of outlier detection are introduced, and a set of potential algorithms is presented. Subsequently, Web-scale computing is introduced in Section \ref{sec2:webscale} to provide models and tools to parallelize and scale the process of outlier detection.

Chapter \ref{chap3:architecture} introduces the architecture which allows the application to run on a cluster of machines distributed across a data-center. The underlying pattern is the Microservice pattern which consists of suites of independently deployable services. Apache Spark is used as the computational platform which enables large-scale processing. Spark also takes care of failing nodes in the cluster by restarting the task onto another worker. Apache Kafka is used as the data-source because of its ability to scale and divide load. Kafka acts as a message queue which integrates nicely with Spark Streaming to apply the algorithm as new messages are appended. All the services are deployed using Docker which enables fast and easy deployment on heterogeneous systems.

Starting with Research sub-question \ref{sub-req1}; \emph{`Which general-purpose outlier detection algorithms can potentially be scaled out'}, a variety of the types and important outlier detection algorithms are given in Table \ref{tbl:overviewAlgorithms}. This boils down to two suitable algorithms in Section \ref{chap4:algorithm}, i.e. Local Outlier Factor (LOF) based algorithms and the Stochastic Outlier Selection (SOS) algorithm. The SOS algorithm is chosen over the LOF algorithm because the latter requires many $k$-nearest neighbour queries which are difficult to implement efficiently in a map-reduce model.

Research sub-question \ref{sub-req2} focuses on the field of big-data and distributed computing to explore the possibilities of scaling; \emph{`Which computational engines are appropriate for outlier-detection in a big-data setting'}. The term `big-data' is an ambiguous one. Therefore, a definition is in place. Section \ref{sec2:webscale} defines it as: `massively parallel software running on tens, hundreds, or even thousands of servers'. Many outlier detection algorithm do not take scalability into account and solely focus on the ability to detect outliers. The present goal is to scale outlier detection on a large pool of easily accessible virtualized resources, which can be dynamically reconfigured in order to adjust to a variable load. The infrastructure given in Chapter \ref{chap3:architecture} enables scalinf the algorithm by adding or removing workers based on load or requirements. Wrapping services inside Docker containers enables fast deployment onto a large number of machines. As the number of machines increases, it becomes more probable that one will fail, therefore availability is guaranteed using Apache Zookeeper, which provides reliable distributed coordination.

The last Research sub-question is \ref{sub-req3}; \emph{`How to adapt the algorithm to work with streams of data, rather than a static set'}. Data is generated faster and real-time data processing is required to act upon what's happening right now. By using the Spark Streaming library and the Apache Kafka data source this can be done, in a micro-batching characteristic. The number of observations which will be taken and the interval between them is defined by a window length. This allows windows to overlap.

Based on the results in Chapter \ref{chap5:results} a number of conclusions can be drawn. We observe that:

\begin{itemize} 
    \item A distributed implementation of outlier-detection works and follows the computational complexity of the algorithm $\mathcal{O}(n^{2})$ as a function of the input size.
    \item The number of partitions within the RDD needs to be tuned to the size of the input and the number of worker nodes to more optimally utilize the resources in the cluster.
    \item The configuration of Apache Spark has to be tuned to the specifications of the systems to avoid unnecessary swapping, which induces significant overhead and slows down the computations.
\end{itemize}

All the computations within Spark are executed in stages. After each stage, synchronization between the workers is done and data is exchanged which is required for the next stage. Based on these observations, the algorithm can be further optimized as explained in the next section.

\section{Further research \label{sec:futherResearch}}
The present research has presented a distributed implementation of outlier detection. This is the first publicly available implementation on top of Apache Spark. The problem lies in the computational complexity of the computation of the distance matrix. By taking the Cartesian product, this is quadratic in respect to the input size. The architecture allows the system to scale, but when the input size gets too large this becomes infeasible.

For further research, the following questions are still open:
\begin{itemize}
    \item Creating a more powerful primitive for the Cartesian product. The computation of the distance matrix is a special case as it takes the Cartesian product of itself. This case might be susceptible to optimization.
    \item Explore the possibility of an optimized data-structure such as the Barnes-Hut tree \cite{Barnes1986} in a distributed fashion. This has been done in a shared memory environment \cite{Dubinski1996132}.
\end{itemize}

